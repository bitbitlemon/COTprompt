***************
** Arguments **
***************
backbone: wide_resnet_28_2
config_file: 
dataset_config_file: configs\datasets\cifar100n.yaml
eval_only: False
load_epoch: None
model_dir: 
no_train: False
opts: []
output_dir: output/cifar100n_vanilla
resume: 
root: e:\COTprompt\datasets\DATA
seed: 1
trainer: Vanilla
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 4
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 256
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 128
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  BEGIN_RATE: 0.3
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  CURRICLUM_EPOCH: 0
  CURRICLUM_MODE: linear
  NAME: CIFAR100N
  NOISE_LABEL: False
  NOISE_RATE: 0.0
  NOISE_TYPE: sym
  NUM_LABELED: -1
  NUM_SHOTS: -1
  PMODE: logP
  REG_E: 0.01
  REG_FEAT: 1.0
  REG_LAB: 1.0
  ROOT: e:\COTprompt\datasets\DATA
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  TARGET_DOMAINS: ()
  USE_OT: False
  VAL_PERCENT: 0.1
  num_class: 100
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bilinear
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.5, 0.5, 0.5]
  PIXEL_STD: [0.5, 0.5, 0.5]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (32, 32)
  TRANSFORMS: ('random_flip', 'random_crop', 'normalize')
MODEL:
  BACKBONE:
    NAME: wide_resnet_28_2
    PRETRAINED: False
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.1
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/cifar100n_vanilla
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 10
  COUNT_ITER: train_x
  PRINT_FREQ: 50
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: Vanilla
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.8.0+cpu
Is debug build: False
CUDA used to build PyTorch: Could not collect
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 11 家庭中文版 (10.0.26100 64 位)
GCC version: Could not collect
Clang version: Could not collect
CMake version: Could not collect
Libc version: N/A

Python version: 3.9.5 (tags/v3.9.5:0a7dcbd, May  3 2021, 17:27:52) [MSC v.1928 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.26100-SP0
Is CUDA available: False
CUDA runtime version: 12.6.20
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3060 Laptop GPU
Nvidia driver version: 581.32
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Name: Intel(R) Core(TM) i7-10870H CPU @ 2.20GHz
Manufacturer: GenuineIntel
Family: 198
Architecture: 9
ProcessorType: 3
DeviceID: CPU0
CurrentClockSpeed: 2208
MaxClockSpeed: 2208
L2CacheSize: 2048
L2CacheSpeed: None
Revision: None

Versions of relevant libraries:
[pip3] flake8==7.3.0
[pip3] numpy==1.23.5
[pip3] onnxruntime==1.19.2
[pip3] optree==0.17.0
[pip3] pytorch-pretrained-bert==0.6.2
[pip3] pytorch-tabnet==4.1.0
[pip3] torch==2.8.0
[pip3] torch-geometric==2.6.1
[pip3] torch_scatter==2.1.2+pt28cpu
[pip3] torchvision==0.23.0
[conda] Could not collect
        Pillow (10.4.0)

Loading trainer: Vanilla
Loading dataset: CIFAR100N
Building transform_train
+ random crop (padding = 4)
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
Building transform_test
+ resize the smaller edge to 32
+ 32x32 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
Data loader size: 390
Data loader size: 40
---------  ---------
Dataset    CIFAR100N
# classes  100
# train_x  50,000
# test     10,000
---------  ---------
Building model
Backbone: wide_resnet_28_2
# params: 1,479,220
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/cifar100n_vanilla\tensorboard)
epoch [1/200] batch [50/390] time 1.220 (2.156) data 0.001 (1.071) loss 4.3734 (4.4363) acc 3.1250 (3.1250) lr 1.0000e-01 eta 1 day, 22:40:55
epoch [1/200] batch [100/390] time 1.112 (1.612) data 0.001 (0.536) loss 4.0314 (4.3038) acc 7.8125 (3.9609) lr 1.0000e-01 eta 1 day, 10:52:43
epoch [1/200] batch [150/390] time 1.072 (1.428) data 0.002 (0.358) loss 3.9655 (4.2125) acc 5.4688 (4.9792) lr 1.0000e-01 eta 1 day, 6:53:12
epoch [1/200] batch [200/390] time 1.029 (1.333) data 0.001 (0.269) loss 3.7476 (4.1377) acc 8.5938 (5.8867) lr 1.0000e-01 eta 1 day, 4:48:04
epoch [1/200] batch [250/390] time 0.994 (1.272) data 0.001 (0.215) loss 3.9369 (4.0804) acc 5.4688 (6.5625) lr 1.0000e-01 eta 1 day, 3:28:29
epoch [1/200] batch [300/390] time 1.046 (1.229) data 0.002 (0.180) loss 3.8361 (4.0354) acc 14.0625 (7.2005) lr 1.0000e-01 eta 1 day, 2:32:11
epoch [1/200] batch [350/390] time 1.031 (1.207) data 0.001 (0.154) loss 3.6900 (3.9954) acc 9.3750 (7.7232) lr 1.0000e-01 eta 1 day, 2:01:52
epoch [2/200] batch [50/390] time 0.968 (2.056) data 0.001 (1.043) loss 3.5509 (3.5949) acc 12.5000 (13.2344) lr 9.9994e-02 eta 1 day, 20:17:54
epoch [2/200] batch [100/390] time 0.981 (1.523) data 0.000 (0.522) loss 3.5542 (3.5817) acc 15.6250 (13.7812) lr 9.9994e-02 eta 1 day, 8:46:54
epoch [2/200] batch [150/390] time 0.894 (1.336) data 0.001 (0.348) loss 3.3898 (3.5545) acc 17.9688 (14.2917) lr 9.9994e-02 eta 1 day, 4:44:15
epoch [2/200] batch [200/390] time 0.968 (1.236) data 0.001 (0.262) loss 3.3561 (3.5340) acc 17.9688 (15.0898) lr 9.9994e-02 eta 1 day, 2:34:36
epoch [2/200] batch [250/390] time 0.935 (1.179) data 0.000 (0.210) loss 3.5613 (3.5053) acc 12.5000 (15.6375) lr 9.9994e-02 eta 1 day, 1:20:02
epoch [2/200] batch [300/390] time 0.903 (1.139) data 0.001 (0.175) loss 3.3303 (3.4692) acc 15.6250 (16.1589) lr 9.9994e-02 eta 1 day, 0:27:02
epoch [2/200] batch [350/390] time 0.946 (1.109) data 0.000 (0.150) loss 3.2884 (3.4391) acc 18.7500 (16.4866) lr 9.9994e-02 eta 23:48:34
epoch [3/200] batch [50/390] time 0.914 (1.588) data 0.003 (0.676) loss 3.1949 (3.1349) acc 22.6562 (21.2344) lr 9.9975e-02 eta 1 day, 10:02:28
epoch [3/200] batch [100/390] time 0.924 (1.249) data 0.001 (0.339) loss 2.9112 (3.0916) acc 23.4375 (22.4219) lr 9.9975e-02 eta 1 day, 2:45:08
epoch [3/200] batch [150/390] time 0.943 (1.143) data 0.001 (0.226) loss 2.9095 (3.0499) acc 21.8750 (23.1719) lr 9.9975e-02 eta 1 day, 0:27:32
epoch [3/200] batch [200/390] time 1.031 (1.100) data 0.002 (0.170) loss 3.0279 (3.0173) acc 21.0938 (23.8672) lr 9.9975e-02 eta 23:31:41
epoch [3/200] batch [250/390] time 0.902 (1.065) data 0.002 (0.136) loss 2.8708 (2.9867) acc 25.7812 (24.4594) lr 9.9975e-02 eta 22:46:39
epoch [3/200] batch [300/390] time 0.923 (1.049) data 0.002 (0.114) loss 2.8694 (2.9598) acc 25.0000 (24.9323) lr 9.9975e-02 eta 22:25:24
epoch [3/200] batch [350/390] time 0.901 (1.029) data 0.002 (0.098) loss 2.7309 (2.9347) acc 40.6250 (25.4219) lr 9.9975e-02 eta 21:58:46
epoch [4/200] batch [50/390] time 0.882 (1.568) data 0.000 (0.666) loss 2.6559 (2.6623) acc 32.0312 (30.3750) lr 9.9944e-02 eta 1 day, 9:27:08
epoch [4/200] batch [100/390] time 0.923 (1.231) data 0.001 (0.334) loss 2.9380 (2.6581) acc 26.5625 (30.4219) lr 9.9944e-02 eta 1 day, 2:14:32
epoch [4/200] batch [150/390] time 0.889 (1.120) data 0.000 (0.223) loss 2.5915 (2.6359) acc 32.0312 (31.0208) lr 9.9944e-02 eta 23:50:43
epoch [4/200] batch [200/390] time 0.887 (1.063) data 0.001 (0.167) loss 2.5529 (2.6338) acc 40.6250 (31.1680) lr 9.9944e-02 eta 22:37:31
epoch [4/200] batch [250/390] time 0.886 (1.029) data 0.000 (0.134) loss 2.4572 (2.6225) acc 36.7188 (31.5219) lr 9.9944e-02 eta 21:53:51
epoch [4/200] batch [300/390] time 0.886 (1.008) data 0.000 (0.112) loss 2.4919 (2.6099) acc 38.2812 (31.7578) lr 9.9944e-02 eta 21:25:16
epoch [4/200] batch [350/390] time 0.867 (0.990) data 0.000 (0.096) loss 2.6687 (2.5991) acc 29.6875 (32.0179) lr 9.9944e-02 eta 21:02:04
epoch [5/200] batch [50/390] time 0.874 (1.494) data 0.002 (0.657) loss 2.4044 (2.3743) acc 37.5000 (35.6875) lr 9.9901e-02 eta 1 day, 7:42:15
epoch [5/200] batch [100/390] time 0.849 (1.172) data 0.001 (0.329) loss 2.4399 (2.3887) acc 38.2812 (36.0078) lr 9.9901e-02 eta 1 day, 0:51:24
epoch [5/200] batch [150/390] time 0.859 (1.068) data 0.001 (0.220) loss 2.5472 (2.3866) acc 28.9062 (35.9427) lr 9.9901e-02 eta 22:37:35
epoch [5/200] batch [200/390] time 0.852 (1.014) data 0.001 (0.165) loss 2.3034 (2.3832) acc 36.7188 (36.1406) lr 9.9901e-02 eta 21:28:19
epoch [5/200] batch [250/390] time 0.855 (0.982) data 0.001 (0.132) loss 2.3603 (2.3758) acc 35.9375 (36.5344) lr 9.9901e-02 eta 20:46:52
epoch [5/200] batch [300/390] time 0.847 (0.962) data 0.002 (0.110) loss 2.5545 (2.3767) acc 31.2500 (36.5833) lr 9.9901e-02 eta 20:20:43
epoch [5/200] batch [350/390] time 0.835 (0.946) data 0.001 (0.095) loss 2.5298 (2.3674) acc 36.7188 (36.8170) lr 9.9901e-02 eta 20:00:12
epoch [6/200] batch [50/390] time 0.891 (1.521) data 0.002 (0.646) loss 2.1046 (2.2043) acc 42.1875 (40.4219) lr 9.9846e-02 eta 1 day, 8:06:33
epoch [6/200] batch [100/390] time 0.935 (1.206) data 0.002 (0.324) loss 2.1790 (2.2283) acc 45.3125 (40.4141) lr 9.9846e-02 eta 1 day, 1:26:42
epoch [6/200] batch [150/390] time 0.903 (1.104) data 0.001 (0.216) loss 1.9405 (2.2312) acc 46.0938 (40.3281) lr 9.9846e-02 eta 23:16:35
epoch [6/200] batch [200/390] time 0.902 (1.056) data 0.001 (0.162) loss 2.3260 (2.2299) acc 35.9375 (40.2305) lr 9.9846e-02 eta 22:14:25
epoch [6/200] batch [250/390] time 0.882 (1.024) data 0.002 (0.130) loss 2.2601 (2.2244) acc 42.9688 (40.3469) lr 9.9846e-02 eta 21:34:07
epoch [6/200] batch [300/390] time 0.864 (1.006) data 0.001 (0.109) loss 2.2739 (2.2150) acc 41.4062 (40.5990) lr 9.9846e-02 eta 21:10:24
epoch [6/200] batch [350/390] time 0.924 (0.990) data 0.000 (0.093) loss 2.3017 (2.2182) acc 40.6250 (40.5692) lr 9.9846e-02 eta 20:48:41
epoch [7/200] batch [50/390] time 0.834 (1.524) data 0.002 (0.642) loss 2.5498 (2.0877) acc 33.5938 (43.6719) lr 9.9778e-02 eta 1 day, 8:00:08
epoch [7/200] batch [100/390] time 0.862 (1.211) data 0.001 (0.322) loss 2.0278 (2.1039) acc 41.4062 (43.2422) lr 9.9778e-02 eta 1 day, 1:24:45
epoch [7/200] batch [150/390] time 0.846 (1.096) data 0.001 (0.215) loss 1.8420 (2.1160) acc 49.2188 (43.0260) lr 9.9778e-02 eta 22:58:56
epoch [7/200] batch [200/390] time 0.856 (1.035) data 0.002 (0.161) loss 2.1505 (2.1153) acc 41.4062 (43.0000) lr 9.9778e-02 eta 21:42:08
epoch [7/200] batch [250/390] time 0.908 (1.007) data 0.002 (0.129) loss 2.3369 (2.1136) acc 42.9688 (43.0562) lr 9.9778e-02 eta 21:05:58
epoch [7/200] batch [300/390] time 0.996 (1.003) data 0.002 (0.108) loss 2.1074 (2.1110) acc 42.9688 (43.1979) lr 9.9778e-02 eta 20:59:26
epoch [7/200] batch [350/390] time 0.878 (0.993) data 0.002 (0.093) loss 2.1823 (2.1113) acc 50.0000 (43.2031) lr 9.9778e-02 eta 20:46:20
epoch [8/200] batch [50/390] time 1.088 (1.704) data 0.000 (0.689) loss 2.2203 (1.9995) acc 39.0625 (45.3438) lr 9.9698e-02 eta 1 day, 11:35:40
epoch [8/200] batch [100/390] time 0.925 (1.336) data 0.002 (0.345) loss 2.0281 (2.0093) acc 43.7500 (45.0625) lr 9.9698e-02 eta 1 day, 3:53:27
epoch [8/200] batch [150/390] time 0.897 (1.195) data 0.001 (0.231) loss 2.0456 (2.0113) acc 42.9688 (45.0000) lr 9.9698e-02 eta 1 day, 0:55:37
epoch [8/200] batch [200/390] time 1.096 (1.166) data 0.003 (0.173) loss 1.9208 (2.0147) acc 50.7812 (45.0117) lr 9.9698e-02 eta 1 day, 0:18:39
epoch [8/200] batch [250/390] time 0.708 (1.114) data 0.001 (0.139) loss 1.9316 (2.0164) acc 44.5312 (44.8813) lr 9.9698e-02 eta 23:13:28
epoch [8/200] batch [300/390] time 0.798 (1.071) data 0.001 (0.116) loss 1.9497 (2.0146) acc 44.5312 (44.9844) lr 9.9698e-02 eta 22:17:46
epoch [8/200] batch [350/390] time 0.736 (1.043) data 0.000 (0.100) loss 2.1714 (2.0180) acc 42.9688 (44.9643) lr 9.9698e-02 eta 21:42:12
epoch [9/200] batch [50/390] time 1.108 (1.678) data 0.002 (0.721) loss 1.6919 (1.8866) acc 53.1250 (48.1094) lr 9.9606e-02 eta 1 day, 10:52:56
epoch [9/200] batch [100/390] time 0.960 (1.340) data 0.002 (0.362) loss 1.8193 (1.9331) acc 46.0938 (47.0469) lr 9.9606e-02 eta 1 day, 3:50:00
epoch [9/200] batch [150/390] time 0.867 (1.192) data 0.003 (0.241) loss 1.8147 (1.9380) acc 48.4375 (46.7708) lr 9.9606e-02 eta 1 day, 0:44:18
epoch [9/200] batch [200/390] time 0.956 (1.097) data 0.001 (0.181) loss 1.9983 (1.9400) acc 44.5312 (46.7539) lr 9.9606e-02 eta 22:45:14
epoch [9/200] batch [250/390] time 0.842 (1.061) data 0.001 (0.145) loss 1.7717 (1.9450) acc 50.0000 (46.7656) lr 9.9606e-02 eta 21:59:47
epoch [9/200] batch [300/390] time 0.864 (1.024) data 0.002 (0.121) loss 2.1197 (1.9449) acc 39.8438 (46.8359) lr 9.9606e-02 eta 21:13:15
epoch [9/200] batch [350/390] time 0.761 (0.998) data 0.002 (0.104) loss 2.1906 (1.9436) acc 38.2812 (46.8036) lr 9.9606e-02 eta 20:40:15
epoch [10/200] batch [50/390] time 0.814 (1.426) data 0.002 (0.655) loss 1.8621 (1.8501) acc 45.3125 (48.6406) lr 9.9501e-02 eta 1 day, 5:29:42
epoch [10/200] batch [100/390] time 0.741 (1.099) data 0.001 (0.328) loss 1.9190 (1.8743) acc 48.4375 (48.3125) lr 9.9501e-02 eta 22:42:19
epoch [10/200] batch [150/390] time 0.787 (1.005) data 0.002 (0.219) loss 1.8602 (1.8827) acc 50.0000 (48.1823) lr 9.9501e-02 eta 20:45:20
epoch [10/200] batch [200/390] time 0.775 (0.949) data 0.001 (0.165) loss 1.8889 (1.8852) acc 50.0000 (48.1914) lr 9.9501e-02 eta 19:34:44
epoch [10/200] batch [250/390] time 0.795 (0.919) data 0.001 (0.132) loss 2.0310 (1.8839) acc 43.7500 (48.2125) lr 9.9501e-02 eta 18:56:43
epoch [10/200] batch [300/390] time 0.844 (0.897) data 0.001 (0.110) loss 1.8840 (1.8909) acc 46.8750 (48.0312) lr 9.9501e-02 eta 18:29:36
epoch [10/200] batch [350/390] time 0.782 (0.880) data 0.001 (0.095) loss 1.8411 (1.8954) acc 51.5625 (47.9643) lr 9.9501e-02 eta 18:07:39
Checkpoint saved to output/cifar100n_vanilla\model\model.pth.tar-10
epoch [11/200] batch [50/390] time 0.954 (1.554) data 0.003 (0.658) loss 1.6048 (1.8141) acc 54.6875 (50.1875) lr 9.9384e-02 eta 1 day, 7:58:05
epoch [11/200] batch [100/390] time 0.810 (1.212) data 0.001 (0.330) loss 1.5930 (1.8463) acc 55.4688 (49.6094) lr 9.9384e-02 eta 1 day, 0:54:22
epoch [11/200] batch [150/390] time 0.912 (1.083) data 0.001 (0.220) loss 1.8643 (1.8471) acc 55.4688 (49.5208) lr 9.9384e-02 eta 22:14:41
epoch [11/200] batch [200/390] time 0.887 (1.017) data 0.001 (0.166) loss 1.7563 (1.8502) acc 52.3438 (49.5078) lr 9.9384e-02 eta 20:52:09
epoch [11/200] batch [250/390] time 0.794 (0.980) data 0.001 (0.133) loss 2.1823 (1.8527) acc 42.9688 (49.4000) lr 9.9384e-02 eta 20:05:44
epoch [11/200] batch [300/390] time 0.823 (0.956) data 0.001 (0.111) loss 1.8849 (1.8554) acc 47.6562 (49.2109) lr 9.9384e-02 eta 19:35:47
epoch [11/200] batch [350/390] time 0.826 (0.935) data 0.001 (0.095) loss 1.8749 (1.8553) acc 46.8750 (49.2254) lr 9.9384e-02 eta 19:09:03
epoch [12/200] batch [50/390] time 0.853 (1.537) data 0.001 (0.691) loss 1.7837 (1.7615) acc 50.7812 (51.0781) lr 9.9255e-02 eta 1 day, 7:26:50
epoch [12/200] batch [100/390] time 0.794 (1.175) data 0.002 (0.346) loss 1.7600 (1.7864) acc 48.4375 (50.4766) lr 9.9255e-02 eta 1 day, 0:01:07
epoch [12/200] batch [150/390] time 0.772 (1.047) data 0.001 (0.231) loss 1.7915 (1.7930) acc 50.7812 (50.4115) lr 9.9255e-02 eta 21:23:02
epoch [12/200] batch [200/390] time 0.769 (0.981) data 0.002 (0.174) loss 1.9280 (1.7980) acc 45.3125 (50.4258) lr 9.9255e-02 eta 20:01:52
epoch [12/200] batch [250/390] time 0.759 (0.942) data 0.000 (0.139) loss 1.8037 (1.7948) acc 48.4375 (50.5562) lr 9.9255e-02 eta 19:13:32
epoch [12/200] batch [300/390] time 0.975 (0.919) data 0.002 (0.116) loss 1.6529 (1.8028) acc 55.4688 (50.4557) lr 9.9255e-02 eta 18:43:58
epoch [12/200] batch [350/390] time 0.794 (0.903) data 0.001 (0.100) loss 1.8586 (1.8090) acc 50.7812 (50.2232) lr 9.9255e-02 eta 18:24:35
epoch [13/200] batch [50/390] time 0.745 (1.424) data 0.001 (0.658) loss 1.6676 (1.7373) acc 51.5625 (52.2344) lr 9.9114e-02 eta 1 day, 4:58:57
epoch [13/200] batch [100/390] time 0.770 (1.098) data 0.002 (0.330) loss 1.9974 (1.7466) acc 41.4062 (51.6875) lr 9.9114e-02 eta 22:20:14
epoch [13/200] batch [150/390] time 0.771 (0.990) data 0.001 (0.220) loss 2.0102 (1.7618) acc 47.6562 (51.3385) lr 9.9114e-02 eta 20:06:53
epoch [13/200] batch [200/390] time 0.760 (0.941) data 0.001 (0.165) loss 1.8575 (1.7605) acc 51.5625 (51.4141) lr 9.9114e-02 eta 19:06:18
epoch [13/200] batch [250/390] time 0.778 (0.909) data 0.002 (0.133) loss 1.6346 (1.7703) acc 55.4688 (51.1469) lr 9.9114e-02 eta 18:26:59
epoch [13/200] batch [300/390] time 0.757 (0.887) data 0.001 (0.111) loss 1.7746 (1.7751) acc 50.7812 (51.1406) lr 9.9114e-02 eta 17:58:53
epoch [13/200] batch [350/390] time 0.761 (0.876) data 0.002 (0.095) loss 1.7790 (1.7739) acc 52.3438 (51.1875) lr 9.9114e-02 eta 17:45:51
epoch [14/200] batch [50/390] time 0.713 (1.430) data 0.001 (0.659) loss 1.7482 (1.7045) acc 55.4688 (52.9531) lr 9.8961e-02 eta 1 day, 4:57:03
epoch [14/200] batch [100/390] time 0.888 (1.106) data 0.001 (0.330) loss 1.5891 (1.6958) acc 53.9062 (52.9141) lr 9.8961e-02 eta 22:22:44
epoch [14/200] batch [150/390] time 0.725 (1.003) data 0.001 (0.220) loss 1.7307 (1.7179) acc 54.6875 (52.5469) lr 9.8961e-02 eta 20:16:23
epoch [14/200] batch [200/390] time 0.789 (0.953) data 0.001 (0.166) loss 1.9320 (1.7264) acc 45.3125 (52.3672) lr 9.8961e-02 eta 19:15:00
epoch [14/200] batch [250/390] time 0.729 (0.920) data 0.001 (0.133) loss 1.6435 (1.7326) acc 50.7812 (52.0438) lr 9.8961e-02 eta 18:34:15
epoch [14/200] batch [300/390] time 0.779 (0.899) data 0.001 (0.111) loss 1.6540 (1.7298) acc 52.3438 (52.0443) lr 9.8961e-02 eta 18:07:57
epoch [14/200] batch [350/390] time 0.832 (0.884) data 0.001 (0.095) loss 1.7270 (1.7358) acc 53.1250 (51.9330) lr 9.8961e-02 eta 17:49:21
epoch [15/200] batch [50/390] time 0.724 (1.430) data 0.000 (0.657) loss 1.7774 (1.6681) acc 50.0000 (53.3594) lr 9.8796e-02 eta 1 day, 4:47:50
epoch [15/200] batch [100/390] time 0.778 (1.113) data 0.002 (0.329) loss 1.6440 (1.6674) acc 53.1250 (53.7578) lr 9.8796e-02 eta 22:24:21
epoch [15/200] batch [150/390] time 0.888 (1.003) data 0.001 (0.220) loss 1.9096 (1.6910) acc 43.7500 (52.9167) lr 9.8796e-02 eta 20:10:11
epoch [15/200] batch [200/390] time 0.944 (0.960) data 0.001 (0.165) loss 1.6227 (1.6995) acc 53.9062 (52.6406) lr 9.8796e-02 eta 19:17:13
epoch [15/200] batch [250/390] time 0.798 (0.926) data 0.001 (0.132) loss 1.7195 (1.7019) acc 54.6875 (52.6000) lr 9.8796e-02 eta 18:35:16
epoch [15/200] batch [300/390] time 0.877 (0.912) data 0.001 (0.110) loss 2.2132 (1.7106) acc 35.1562 (52.4427) lr 9.8796e-02 eta 18:18:24
epoch [15/200] batch [350/390] time 0.771 (0.913) data 0.001 (0.095) loss 1.7573 (1.7135) acc 46.8750 (52.3951) lr 9.8796e-02 eta 18:19:02
epoch [16/200] batch [50/390] time 0.756 (1.460) data 0.002 (0.658) loss 1.9084 (1.6488) acc 46.0938 (53.5469) lr 9.8618e-02 eta 1 day, 5:14:29
epoch [16/200] batch [100/390] time 0.749 (1.134) data 0.002 (0.330) loss 1.6998 (1.6600) acc 56.2500 (53.7188) lr 9.8618e-02 eta 22:41:45
epoch [16/200] batch [150/390] time 0.988 (1.022) data 0.002 (0.220) loss 1.9812 (1.6622) acc 46.0938 (53.6146) lr 9.8618e-02 eta 20:26:01
epoch [16/200] batch [200/390] time 0.855 (0.964) data 0.002 (0.165) loss 1.7539 (1.6725) acc 53.1250 (53.5078) lr 9.8618e-02 eta 19:15:33
epoch [16/200] batch [250/390] time 0.799 (0.931) data 0.002 (0.133) loss 1.5847 (1.6812) acc 56.2500 (53.2437) lr 9.8618e-02 eta 18:36:09
epoch [16/200] batch [300/390] time 0.800 (0.906) data 0.001 (0.111) loss 1.6897 (1.6828) acc 53.9062 (53.2292) lr 9.8618e-02 eta 18:05:04
epoch [16/200] batch [350/390] time 0.771 (0.889) data 0.001 (0.095) loss 1.8509 (1.6919) acc 47.6562 (53.0491) lr 9.8618e-02 eta 17:43:16
epoch [17/200] batch [50/390] time 0.736 (1.426) data 0.000 (0.659) loss 1.6264 (1.6086) acc 56.2500 (54.9219) lr 9.8429e-02 eta 1 day, 4:24:13
epoch [17/200] batch [100/390] time 0.777 (1.106) data 0.001 (0.330) loss 1.6659 (1.6428) acc 49.2188 (53.8047) lr 9.8429e-02 eta 22:00:58
epoch [17/200] batch [150/390] time 1.187 (1.039) data 0.001 (0.221) loss 1.7109 (1.6520) acc 52.3438 (53.8854) lr 9.8429e-02 eta 20:39:54
epoch [17/200] batch [200/390] time 1.338 (1.062) data 0.002 (0.166) loss 1.4843 (1.6623) acc 60.1562 (53.6641) lr 9.8429e-02 eta 21:06:31
epoch [17/200] batch [250/390] time 0.990 (1.069) data 0.000 (0.133) loss 1.4359 (1.6645) acc 60.9375 (53.7125) lr 9.8429e-02 eta 21:14:33
epoch [17/200] batch [300/390] time 0.988 (1.062) data 0.001 (0.111) loss 1.6661 (1.6618) acc 48.4375 (53.6328) lr 9.8429e-02 eta 21:05:02
epoch [17/200] batch [350/390] time 0.981 (1.057) data 0.000 (0.095) loss 1.8316 (1.6669) acc 50.7812 (53.5491) lr 9.8429e-02 eta 20:57:59
epoch [18/200] batch [50/390] time 0.969 (1.763) data 0.001 (0.757) loss 1.9493 (1.6346) acc 46.8750 (54.2812) lr 9.8228e-02 eta 1 day, 10:55:01
epoch [18/200] batch [100/390] time 1.026 (1.383) data 0.002 (0.379) loss 1.5761 (1.6229) acc 54.6875 (54.8828) lr 9.8228e-02 eta 1 day, 3:22:10
epoch [18/200] batch [150/390] time 1.012 (1.269) data 0.001 (0.253) loss 1.4907 (1.6348) acc 57.8125 (54.6875) lr 9.8228e-02 eta 1 day, 1:06:00
epoch [18/200] batch [200/390] time 1.106 (1.209) data 0.001 (0.190) loss 1.5094 (1.6398) acc 62.5000 (54.5547) lr 9.8228e-02 eta 23:53:30
epoch [18/200] batch [250/390] time 0.999 (1.175) data 0.001 (0.153) loss 1.5635 (1.6448) acc 60.1562 (54.3781) lr 9.8228e-02 eta 23:13:15
epoch [18/200] batch [300/390] time 1.045 (1.159) data 0.002 (0.127) loss 1.5811 (1.6489) acc 56.2500 (54.1771) lr 9.8228e-02 eta 22:52:24
epoch [18/200] batch [350/390] time 0.971 (1.133) data 0.001 (0.109) loss 1.3468 (1.6524) acc 63.2812 (54.1741) lr 9.8228e-02 eta 22:20:34
epoch [19/200] batch [50/390] time 1.061 (1.758) data 0.000 (0.757) loss 1.5078 (1.5922) acc 57.8125 (55.5781) lr 9.8015e-02 eta 1 day, 10:37:47
epoch [19/200] batch [100/390] time 0.963 (1.376) data 0.002 (0.379) loss 1.7638 (1.5965) acc 47.6562 (55.4141) lr 9.8015e-02 eta 1 day, 3:05:58
epoch [19/200] batch [150/390] time 1.019 (1.254) data 0.001 (0.253) loss 1.8345 (1.6058) acc 44.5312 (54.9635) lr 9.8015e-02 eta 1 day, 0:40:49
epoch [19/200] batch [200/390] time 1.078 (1.187) data 0.001 (0.191) loss 1.7439 (1.6104) acc 56.2500 (54.7773) lr 9.8015e-02 eta 23:20:21
epoch [19/200] batch [250/390] time 0.958 (1.150) data 0.000 (0.153) loss 1.7082 (1.6213) acc 54.6875 (54.5375) lr 9.8015e-02 eta 22:35:28
epoch [19/200] batch [300/390] time 1.014 (1.127) data 0.002 (0.127) loss 1.6017 (1.6292) acc 52.3438 (54.2839) lr 9.8015e-02 eta 22:07:23
epoch [19/200] batch [350/390] time 1.283 (1.114) data 0.002 (0.109) loss 1.5818 (1.6302) acc 53.9062 (54.2545) lr 9.8015e-02 eta 21:51:01
epoch [20/200] batch [50/390] time 1.177 (1.765) data 0.002 (0.762) loss 1.7568 (1.5456) acc 50.0000 (56.5469) lr 9.7790e-02 eta 1 day, 10:35:08
epoch [20/200] batch [100/390] time 1.038 (1.393) data 0.001 (0.382) loss 1.8741 (1.5748) acc 48.4375 (55.3281) lr 9.7790e-02 eta 1 day, 3:16:55
epoch [20/200] batch [150/390] time 0.979 (1.261) data 0.003 (0.255) loss 1.7251 (1.5889) acc 56.2500 (55.3646) lr 9.7790e-02 eta 1 day, 0:39:57
epoch [20/200] batch [200/390] time 1.028 (1.197) data 0.001 (0.192) loss 1.5033 (1.5994) acc 56.2500 (55.1484) lr 9.7790e-02 eta 23:24:17
epoch [20/200] batch [250/390] time 0.859 (1.156) data 0.002 (0.154) loss 1.5568 (1.6097) acc 54.6875 (54.9406) lr 9.7790e-02 eta 22:34:49
epoch [20/200] batch [300/390] time 1.005 (1.129) data 0.001 (0.128) loss 1.6395 (1.6204) acc 55.4688 (54.6224) lr 9.7790e-02 eta 22:02:13
epoch [20/200] batch [350/390] time 1.008 (1.113) data 0.003 (0.110) loss 1.5930 (1.6264) acc 57.0312 (54.4844) lr 9.7790e-02 eta 21:42:27
Checkpoint saved to output/cifar100n_vanilla\model\model.pth.tar-20
epoch [21/200] batch [50/390] time 0.985 (1.763) data 0.002 (0.755) loss 1.4193 (1.5153) acc 62.5000 (57.0469) lr 9.7553e-02 eta 1 day, 10:21:25
epoch [21/200] batch [100/390] time 1.034 (1.377) data 0.001 (0.378) loss 1.5914 (1.5445) acc 47.6562 (56.3984) lr 9.7553e-02 eta 1 day, 2:48:51
epoch [21/200] batch [150/390] time 1.115 (1.261) data 0.001 (0.253) loss 1.5131 (1.5667) acc 62.5000 (55.8594) lr 9.7553e-02 eta 1 day, 0:32:35
